{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>제목</th>\n",
       "      <th>날짜</th>\n",
       "      <th>추천수</th>\n",
       "      <th>조회수</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>법원 \"교통사고 가해측 보험사는 피해차 가치 하락도 배상해야\"</td>\n",
       "      <td>05/03</td>\n",
       "      <td>5.0</td>\n",
       "      <td>692.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>에디슨EV \"채권자가 법원에 파산 신청…채권액 36억원\"</td>\n",
       "      <td>05/06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>기아, 신형 니로 EV 사전계약 실시…1회 충전에 401㎞ 주..</td>\n",
       "      <td>05/03</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1452.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>현대차, 3년 만에 유관중으로 `현대 N 페스티벌` 개최</td>\n",
       "      <td>05/03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>호세 무뇨스 현대차 사장 이사회 합류…글로벌시장 챙긴다</td>\n",
       "      <td>05/04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>274.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     제목     날짜  추천수     조회수\n",
       "0    법원 \"교통사고 가해측 보험사는 피해차 가치 하락도 배상해야\"  05/03  5.0   692.0\n",
       "1       에디슨EV \"채권자가 법원에 파산 신청…채권액 36억원\"  05/06  3.0  1266.0\n",
       "2  기아, 신형 니로 EV 사전계약 실시…1회 충전에 401㎞ 주..  05/03  2.0  1452.0\n",
       "3       현대차, 3년 만에 유관중으로 `현대 N 페스티벌` 개최  05/03  1.0   260.0\n",
       "4        호세 무뇨스 현대차 사장 이사회 합류…글로벌시장 챙긴다  05/04  1.0   274.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "news_df = pd.read_csv(\"./보배드림 뉴스 크롤링.csv\")\n",
    "news_df = news_df.drop(['내용'], axis=1)\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eunjeon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01meunjeon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mecab\n\u001b[0;32m      3\u001b[0m tagger\u001b[38;5;241m=\u001b[39mMecab()\n\u001b[0;32m      4\u001b[0m significant_tags \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNNG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNNP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNNB\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAG\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAJ\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXSV\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXSA\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'eunjeon'"
     ]
    }
   ],
   "source": [
    "from eunjeon import Mecab\n",
    "\n",
    "tagger=Mecab()\n",
    "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
    "\n",
    "def tagging_text(texts):\n",
    "    corpus = []\n",
    "    for text in texts:\n",
    "        text = tagger.pos(text)\n",
    "        for i in range(len(text)-1):\n",
    "            if text[i][1] in significant_tags:\n",
    "                corpus.append(f\"{text[i][0]}/{text[i][1]}\")\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger.pos(\"교통사고 가해측\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_corpus = tagging_text(news_df.제목)\n",
    "\n",
    "for i in range(5):\n",
    "    print(tagged_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
    "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
    "p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
    "p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        ori_sent = sent\n",
    "        mached_terms = re.findall(p1, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        mached_terms = re.findall(p2, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                if tag != 'VX':\n",
    "                    modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p3, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p4, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        corpus.append(ori_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_corpus = stemming_text(tagged_corpus)\n",
    "\n",
    "for i in range(0, 5):\n",
    "    print(stem_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['이/VCP','있/VA','하/VV','씨/NNB','것/NNB','들/XSN','그/MM','되/VV','수/NNB','이/NP','속/NNG',\n",
    "'보/VX','않/VX','집/NNG','없/VA','살/VV','나/NP','적/XSN','주/VV','월/NNB','데/NNB','등/NNB','같/VA','안/MAG',\n",
    "'우리/NP','어떤/MM','때/NNG','내/NP','년/NNB','내/VV','가/VV''한/MM','명/NNB','지/VX','오/VV','말/NNG','일/NNG',\n",
    "'앞/NNG','번/NNB','나/VX','두/VV','알/VV','개/NNB','받/VV','전/NNG','들/VV','일/NNB','또/MAG','점/NNG','싶/VX',\n",
    "'더/MAG','말/VX','많/VA','좀/MAG','원/NNB','좋/VA','잘/MAG','크/VA','중/NNB','놓/VX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        modi_sent = []\n",
    "        for word in sent.split(' '):\n",
    "            if word not in stopwords:\n",
    "                modi_sent.append(word)\n",
    "        corpus.append(' '.join(modi_sent))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_stopword_corpus = remove_stopword_text(stem_corpus)\n",
    "\n",
    "for i in range(5):\n",
    "    print(removed_stopword_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_df = pd.read_excel(\"./대화_데이터셋.xlsx\")\n",
    "comm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(comm_df, test_size=0.15, random_state=3)\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Emotion'].value_counts().plot(kind = 'bar')\n",
    "print(train.groupby('Emotion').size().reset_index(name = 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train['Sentence'] = train['Sentence'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "test['Sentence'] = test['Sentence'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train['Sentence'] = train['Sentence'].str.replace('^ +', \"\")\n",
    "test['Sentence'] = test['Sentence'].str.replace('^ +', \"\")\n",
    "train['Sentence'].replace('', np.nan, inplace = True)\n",
    "test['Sentence'].replace('', np.nan, inplace = True)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "stopwords = ['도', '는', '다', '의', '가', '이', '은', '한', '에', '하', '고', '을', '를', '인', '듯', '과', '와', '네', '들', '듯', '지', '임', '게']\n",
    "okt = Okt()\n",
    "t_train = [] \n",
    "t_test = []\n",
    "\n",
    "for sentence in train['Sentence']:\n",
    "    temp_X = okt.morphs(str(sentence), stem = True) \n",
    "    temp_X = [word for word in temp_X if not word in stopwords]\n",
    "    t_train.append(temp_X)\n",
    "    \n",
    "for sentence in test['Sentence']:\n",
    "    temp_X = okt.morphs(str(sentence), stem = True)\n",
    "    temp_X = [word for word in temp_X if not word in stopwords]\n",
    "    t_test.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(t_train)\n",
    "tokenizer.fit_on_texts(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    " \n",
    "with open(\"comm_data.pickle\",\"wb\") as fw:\n",
    "    pickle.dump(tokenizer, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_train1 = tokenizer.texts_to_sequences(t_train)\n",
    "t_test1 = tokenizer.texts_to_sequences(t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_train = train['Emotion']\n",
    "e_test = test['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "max_len = max(len(l) for l in t_train1)\n",
    "print(vocab_size)\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t_train2 = pad_sequences(t_train1, maxlen = max_len)\n",
    "t_test2 = pad_sequences(t_test1, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(LSTM(128, return_sequences = True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(GRU(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='LDGD', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('SC.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['acc'])\n",
    "history = model.fit(t_train2, e_train, epochs=15, callbacks=[es, mc], batch_size=6000, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "1.19.5\n",
    "print(np.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('LDGD.h5')\n",
    "print(loaded_model.evaluate(t_test2, e_test)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "e_train_pred = cross_val_predict(loaded_model, t_train2, e_train, cv=3)\n",
    "cf = confusion_matrix(e_train, e_train_pred)\n",
    "print(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "263930470851f494f0ed2879c35b57985588df20f9e529b86e97dd5eb9ddc466"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
