{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from hanspell import spell_checker\n",
    "from konlpy.tag import Okt, Hannanum\n",
    "import urllib.request\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, MaxPooling1D, LSTM\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from krwordrank.sentence import summarize_with_sentences\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from IPython.display import Image\n",
    "from mlxtend.plotting import scatterplotmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonata = pd.read_csv('/Users/parkjubro/Desktop/파이널/합본1/현대+쏘나타_label.csv')\n",
    "sonata = sonata.drop('Unnamed: 0', axis = 'columns')\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def preprocessing1(text):\n",
    "\n",
    "    #이모티콘 제거 (아이폰 이모티콘들은 따로 코드가 존재)\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00010000-\\U0010FFFF\"                   \n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    #분석에 어긋나는 불용어구 제외 (특수문자, 의성어)\n",
    "    han = re.compile(r'[ㄱ-ㅎㅏ-ㅣ!?~,\".\\n\\r#\\ufeff\\u200d\\xa0]')\n",
    "    \n",
    "    # 전처리 1 (댓글 이모지 밑 기호 정리 후 정규화)\n",
    "    comment_result = [] \n",
    "\n",
    "    for i in text:\n",
    "        tokens = re.sub(emoji_pattern,\"\",i) # 이모지 패턴 적용\n",
    "        tokens = re.sub(han,\"\",tokens) # han 적용\n",
    "        tokens = re.sub('[-=+,#/\\?:^.@*\\\"※%~∼ㆍ!【】』㈜©囹圄秋 ■◆◇▷▶◁◀ △▲▽▼<>‘|\\(\\)\\[\\]`\\'…》→←↑↓↔〓♤♠♡♥♧♣⊙◈▣◐◑☆★\\”\\“\\’·※~ ! @ # $ % ^ & * \\ \" ]', ' ', tokens)\n",
    "        # 기타 특수문자들 제거\n",
    "        tokens = okt.normalize(tokens) # 정규화\n",
    "        comment_result.append(tokens)\n",
    "\n",
    "    # 전처리 2 (spell_checker 활용하여 오탈자 수정)\n",
    "    checked_list = [] \n",
    "    \n",
    "    for comment in tqdm(comment_result):\n",
    "        sent = comment\n",
    "        try:\n",
    "            spelled_sent = spell_checker.check(sent)\n",
    "            checked_sent = spelled_sent.checked\n",
    "            checked_list.append(checked_sent)\n",
    "        except:\n",
    "            print(sent)\n",
    "            checked_list.append(sent)\n",
    "    \n",
    "    return checked_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checked_list = preprocessing1(list(sonata['comments']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위해 댓글과 라벨 컬럼만 있는 새로운 샘플 데이터프레임 생성\n",
    "sample = pd.DataFrame() \n",
    "sample['comments'] = checked_list\n",
    "sample['label'] = sonata['label']\n",
    "# trian data 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_encoded = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# w2index\n",
    "word_to_index = tokenizer.word_index\n",
    "\n",
    "# 댓글 최대 길이에 따라 훈련 데이터 크기 설정 \n",
    "long = max(len(sample) for sample in X_train_encoded) # 전처리 과정에서 댓글 최대 길이가 가끔 달라지기 때문에 long 변수 설정함\n",
    "max_len = long\n",
    "X_train_padded = pad_sequences(X_train_encoded, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "dropout_ratio = 0.3\n",
    "num_filters = 32\n",
    "kernel_size = 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(Dropout(dropout_ratio))\n",
    "model.add(Conv1D(num_filters, kernel_size, padding='valid', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(dropout_ratio)) # 과적합 방지를 위해 validation loss가 3번 증가할 때 자동으로 학습 중지\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor = 'val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train_padded, y_train, epochs=13, batch_size=137, validation_split=0.2, callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('/Users/parkjubro/Desktop/파이널/데이터들/test.csv').astype(str)\n",
    "x_test = test_df['comments'].tolist()\n",
    "\n",
    "vocab_size = 30000\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_test)\n",
    "X_test_encoded = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "test_long = max(len(sample) for sample in X_test_encoded)\n",
    "X_test_padded = pad_sequences(X_test_encoded, maxlen = test_long)\n",
    "\n",
    "test_df['label'] = prediction\n",
    "test_df = test_df.drop(columns = 'Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "texts = test_df['comments'].tolist()\n",
    "penalty = lambda x:0 if (10 <= len(x) <= 150) else 1\n",
    "stopwords = {'너무', '리뷰', '진짜', '기자님', '정말', '많이', '영상', '보고', \n",
    "             '이번', '같은', '그냥', '시승기', '보면', '하는', '김한용', '근데', \n",
    "             '아니', '다른', '역시', '항상', '한상기', '라이', '있는', '생각', \n",
    "             '정도', '나오', '지금', '같아', '이제', '있습니다', '요즘', '그리고',\n",
    "            '어떤', '이쁘', '보이', '보니', '때문에', '솔직히', '뭔가', '설명',\n",
    "            '같네요', '하고', '감사', '특히', '봤습니', '정도', '차가', '차를', \n",
    "             '엄청', '이런', '보는', '훨씬', '까요', '아닌', '20', '현대', '벤츠',\n",
    "             '없는', '현기', '한국', '그런', '10', '아반떼', '운전', '하나', '저도',\n",
    "             '저는', '이거', '소리', '이상', '그래', '차는', '있어', '이렇게', '아직',\n",
    "             '들어', '만들', '타는', '합니다', '우리', '차이' ,'일본', '아우디', '볼보', \n",
    "            '갑니다', '어떻게', '다시', '구매', '제가', '가장', '조금', '하면', '미국',\n",
    "             '국내', '한번', '기아', '신형', '그랜저', '쌍용', '30', '거의', '하지', 'the',\n",
    "             '스포', '사는', '많은', '봤습니다', '없어', '바로', '되는', '혹시', '계속',\n",
    "             '확실히', '같네', '텐데', '건가', '그렇', '오늘', '무슨', '국산', '해도', '없고',\n",
    "            '광고', '대한', '아주', '싶네요', '부분', '얼마나', '제일',' 아무리', '궁금', '모르',\n",
    "            '개인', '아무리', '타고', '것도', '나온'}\n",
    "\n",
    "keywords, sents = summarize_with_sentences(\n",
    "    texts,\n",
    "    penalty=penalty,\n",
    "    stopwords = stopwords,\n",
    "    diversity=0.5,\n",
    "    num_keywords=100,\n",
    "    num_keysents=10, # 키워드 분석을 통해 가장 핵심적인 문장들 뽑아내는 개수\n",
    "    verbose=False\n",
    ")\n",
    "for word, r in sorted(keywords.items(), key = lambda x:x[1], reverse=True)[:30]:\n",
    "                     print('%8s:\\t%.4f' % (word, r))\n",
    "                     new_label = []\n",
    "                     \n",
    "for comment in sample['comments']:\n",
    "    if comment in checked_list:\n",
    "        new_label.append(0)\n",
    "    else:\n",
    "        new_label.append(1)\n",
    "\n",
    "sonata['new_label'] = new_label\n",
    "sonata['new_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_df = pd.read_csv('/Users/parkjubro/Desktop/파이널/데이터들/preprocessed_kb_0518.csv')\n",
    "df2 = car_df.drop_duplicates() # 중복값 제거\n",
    "df3 = df2[['car_name','depreciation','year','use','mileage','car_type','insurance']] # 차량모델, 가격, 연식, 사용연수, 주행거리, 차종, 사고여부만 뽑음 (중요한 영향변수)\n",
    "df3 = df3.reset_index() # 인덱스 초기화\n",
    "df3 = df3.drop(columns = 'index', axis = 1)\n",
    "df5 = pd.get_dummies(data = df4, columns = ['car_type', 'car_name', 'insurance']) # 차 종류, 차량 모델, 사고여부 원-핫인코딩\n",
    "\n",
    "std = StandardScaler()\n",
    "\n",
    "train_scaled = std.fit_transform(df5)\n",
    "\n",
    "x_data = df5.drop(columns = 'depreciation')\n",
    "y_data = df5['depreciation']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2 , random_state = 0)\n",
    "\n",
    "linear = LinearRegression()\n",
    "linear.fit(x_train, y_train)\n",
    "rfr = RandomForestRegressor(random_state = 0)\n",
    "rfr.fit(x_train, y_train)\n",
    "\n",
    "print(linear.score(x_train, y_train))\n",
    "print(cross_val_score(linear, x_train, y_train, cv = 3))\n",
    "print(linear.score(x_test, y_test))\n",
    "print(rfr.score(x_train, y_train))\n",
    "print(cross_val_score(rfr, x_train, y_train, cv = 3))\n",
    "print(rfr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#수치형 변수들만 가지고 모델링\n",
    "#kb에 use, mileage, year, new_price, depreciation, forecast_min, forecast_max, car_cc 포함\n",
    "y=kb[['price']].to_numpy()\n",
    "kb=kb.drop(columns=['price','trans','loss','flood','usage','change','insurance','sales_corp','sales_loca','options','car_area','car_no','car_brand','car_name','name_datailed','fuel','car_type','color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test, train data 분리\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.85, random_state=1)\n",
    "\n",
    "lr = LinearRegression(fit_intercept = True, normalize= True, copy_X=True)\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "gb = GradientBoostingRegressor(min_samples_leaf=10, min_samples_split=5,learning_rate=0.5,max_depth=3, n_estimators=1000)\n",
    "gb.fit(x_train, y_train)\n",
    "\n",
    "#다중회귀\n",
    "lm = LinearRegression()\n",
    "X = kb[['mileage']]\n",
    "Y = kb['price']\n",
    "Z = kb[['year', 'use', 'depreciation', 'new_price']]\n",
    "lm.fit(X,Y)\n",
    "lm.fit(Z, kb['price'])\n",
    "Y = lm.predict(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 영화 리뷰 데이터\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\", filename=\"ratings_train.txt\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")\n",
    "\n",
    "train_data_movie = pd.read_table('ratings_train.txt')\n",
    "test_data_movie = pd.read_table('ratings_test.txt')\n",
    "\n",
    "# 네이버 쇼핑 리뷰 데이터\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/bab2min/corpus/master/sentiment/naver_shopping.txt\", filename=\"ratings_total.txt\")\n",
    "\n",
    "total_data = total_data.rename(columns={'reviews':'document'})\n",
    "total_data['label'] = np.select([total_data.ratings > 3], [1], default=0)\n",
    "train_data = pd.concat([train_data_movie,train_data_shopping])\n",
    "train_data.reset_index()\n",
    "\n",
    "# test_data 합치기\n",
    "test_data = pd.concat([test_data_movie,test_data_shopping])\n",
    "test_data = test_data.reset_index()\n",
    "\n",
    "# document 열의 중복 제거\n",
    "train_data.drop_duplicates(subset=['document'], inplace=True)\n",
    "\n",
    "train_data.loc[train_data.document.isnull()]\n",
    "train_data = train_data.dropna(subset=['document']) # Null 값이 존재하는 행 제거\n",
    "\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "train_data['document'] = train_data['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "train_data['document'].replace('', np.nan, inplace=True)\n",
    "train_data = train_data.dropna(subset=['document'])\n",
    "\n",
    "test_data.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "test_data['document'] = test_data['document'].str.replace(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "test_data['document'] = test_data['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "test_data['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "test_data = test_data.dropna(subset=['document']) # Null 값 제거\n",
    "\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "X_train = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_train.append(stopwords_removed_sentence)\n",
    "\n",
    "X_test = []\n",
    "for sentence in tqdm(test_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    X_test.append(stopwords_removed_sentence)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "# 전체 단어 개수 중 빈도수 2이하인 단어는 제거.\n",
    "# 0번 패딩 토큰을 고려하여 + 1\n",
    "vocab_size = total_cnt - rare_cnt + 1\n",
    "\n",
    "tokenizer = Tokenizer(vocab_size) \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "\n",
    "y_train = np.array(train_data['label'])\n",
    "y_test = np.array(test_data['label'])\n",
    "drop_train = [index for index, sentence in enumerate(X_train) if len(sentence) < 1]\n",
    "\n",
    "# 빈 샘플들을 제거\n",
    "X_train = np.delete(X_train, drop_train, axis=0)\n",
    "y_train = np.delete(y_train, drop_train, axis=0)\n",
    "\n",
    "def below_threshold_len(max_len, nested_list):\n",
    "  count = 0\n",
    "  for sentence in nested_list:\n",
    "    if(len(sentence) <= max_len):\n",
    "        count = count + 1\n",
    "\n",
    "max_len = 30\n",
    "below_threshold_len(max_len, X_train)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "hidden_units = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('best_model.h5')\n",
    "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_predict(new_sentence):\n",
    "  new_sentence = re.sub(r'[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]','', new_sentence)\n",
    "  new_sentence = okt.morphs(new_sentence, stem=True) # 토큰화\n",
    "  new_sentence = [word for word in new_sentence if not word in stopwords] # 불용어 제거\n",
    "  encoded = tokenizer.texts_to_sequences([new_sentence]) # 정수 인코딩\n",
    "  pad_new = pad_sequences(encoded, maxlen = max_len) # 패딩\n",
    "  score = float(loaded_model.predict(pad_new)) # 예측\n",
    "  if(score > 0.5):\n",
    "    labeling = 1\n",
    "  else:\n",
    "    labeling = 0\n",
    "\n",
    "  return labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(df,df_col):\n",
    "    \n",
    "    df['labeling'] = ''\n",
    "\n",
    "    for idx, comment in enumerate(df_col):\n",
    "        labeling = sentiment_predict(comment)\n",
    "        df['labeling'][idx] = labeling\n",
    "\n",
    "    df['comments'] = df['comments'].str.replace(\"[^A-Za-z0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "    \n",
    "    return df['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_df = labeling(youtube_df, youtube_df['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(df_col):\n",
    "  hanspell_sent_lst = []\n",
    "\n",
    "  for i in df_col[:100]:\n",
    "      \n",
    "    spelled_sent = spell_checker.check(i) # 맞춤법 검사\n",
    "    hanspell_sent = spelled_sent.checked # 띄어쓰기 교정\n",
    "    hanspell_sent_lst.append(hanspell_sent)\n",
    "\n",
    "  # 불용어 설정\n",
    "  stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "  han = Hannanum()\n",
    "\n",
    "  han_nouns = []\n",
    "\n",
    "  for i in hanspell_sent_lst:\n",
    "      tokenized_sentence = han.nouns(i)\n",
    "      stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "      han_nouns.append(stopwords_removed_sentence)\n",
    "\n",
    "      # 글자수 한자리 단어, 'ㅋ'이 포함된 단어은 불용어 사전에 넣기\n",
    "      for j in stopwords_removed_sentence:\n",
    "        if len(j) <= 1 or 'ㅋ' in j:\n",
    "          stopwords.append(j)\n",
    "  \n",
    "  return han_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_review_han_nouns = filtering(youtube_df['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = sum(youtube_review_han_nouns, [])\n",
    "count = Counter(word_list)\n",
    "word_count = dict(count.most_common())\n",
    "\n",
    "# 로컬에 있는 폰트를 사용할 경우, 폰트의 경로를 font_path에 추가 해주시면 됩니다.\n",
    "wc = WordCloud(font_path=fontpath, background_color = 'white',colormap=matplotlib.cm.inferno,  max_words=100, width=800, height=800, prefer_horizontal = True)\n",
    "cloud = wc.fit_words(word_count)\n",
    "cloud.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = car_df.drop(columns=['price','car_area','car_no', 'car_brand', 'car_name', 'name_datailed', 'fuel', 'car_type', 'color', 'trans', 'loss', 'flood', 'usage','insurance', 'sales_corp', 'sales_loca', 'options'])\n",
    "y = car_df[['price']]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=True, normalize = True, copy_X = True)\n",
    "lr.fit(x_train, y_train)\n",
    "y_predict = lr.predict(x_test)\n",
    "print('LinearRegression trian 정확도 :', lr.score(x_train, y_train))\n",
    "print('LinearRegression test 정확도:', lr.score(x_test, y_test))\n",
    "print('LinearRegression test 정확도:', r2_score(y_test, lr.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(min_samples_leaf=10, min_samples_split=5, learning_rate=0.5, max_depth=3, n_estimators=1000)\n",
    "gb.fit(x_train, y_train)\n",
    "y_gb_predict = gb.predict(x_test)\n",
    "\n",
    "print('gb train 정확도:', gb.score(x_train, y_train))\n",
    "print('gb test 정확도:', gb.score(x_test, y_test))\n",
    "print('gb test 정확도:', r2_score(y_test, gb.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_gb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestRegressor(n_estimators = 50, random_state  = 42)\n",
    "rf_clf.fit(x_train, y_train)\n",
    "pred = rf_clf.predict(x_test)\n",
    "\n",
    "print('RandomForest train 정확도:', rf_clf.score(x_train, y_train))\n",
    "print('RandomForest test 정확도:', rf_clf.score(x_test, y_test))\n",
    "print('RandomForest test 정확도:', r2_score(y_test, rf_clf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "\n",
    "xgb_model.fit(x_train,y_train)\n",
    "predictions = xgb_model.predict(x_test)\n",
    "\n",
    "r_sq = xgb_model.score(x_train, y_train)\n",
    "print('XBoost train 정확도: ',r_sq)\n",
    "print('XBoost test 정확도: ',explained_variance_score(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
